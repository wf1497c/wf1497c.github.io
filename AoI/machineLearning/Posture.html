<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Louis Tang</title><meta name="author" content="Louis Tang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Louis Tang</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/AoI"> Research</a></li><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Exp"> Experience</a></li><li class="menus_item"><a class="site-page" href="/Contact"> Contact</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Louis Tang</h3><p class="author-bio">Research Personal Website</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><html>
    <head>
        <style>
            .flex {
              display: flex;
              padding: 5px;
              <!--background-color: #003C9D;-->
              // 修改以下值試試看 row | row-reverse | column | column-reverse;
              flex-direction: row;
              flex-wrap: wrap;
              word-wrap:break-word;
            }
            .item {
                width: 450px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: top;
                justify-content: left;
                color: black;
                font-size: 2rem;
                <!-- background-color: #4F4F4F; -->
            }
            .imgdes {
                width: 360px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: center;
                justify-content: left;
                color: black;
                font-size: 2rem;
                text-align: left;
                word-wrap:break-word
                <!-- background-color: #4F4F4F; -->
            }
            .item2 {
                height: 400px;
                width: 500px;
                background-color: #5B00AE;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                line-height: 30px;
            }
            .item3 {
                height: 400px;
                width: 500px;
                background-color: #4F9D9D;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                padding-right: 20px;
                line-height: 30px;
            }
            h2 {
                margin: 0;
            }
            hr {
                margin-top: 20px;
                margin-bottom: 50px;
                margin-right: 20px;
            }
            a {
                 style="color:red;"
            }
            li {
              color: black;
              font-size: 1em;
              list-style:none
            }
            ul {
                margin: 0px;
            }
            br {
                line-height: 10%;
                font-size: 24%;
            }
            .page article {
                padding-left: 70px;
                padding-right: 0px;
            }
            h1 {
                margin-bottom: 30px;
            }         
            p{
              word-wrap:break-word; 
              width: 900;
            }   
        </style>
    </head>
    <body>
        <h1><font face="TimesNewRoman" size=7> Sitting Postures Recognition </font></h1>
        <hr></hr>
        <p><font face="TimesNewRoman" size=5>(Watch my presentation at 4th GCBME to understand this topic ↓)
        <br></br>
        <iframe src="https://drive.google.com/file/d/1TX4AsQJBE7JrQWObd2mpfWHSJ-1p8bv1/preview" scrolling="no" frameborder="0" width="100%" height="430" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>
        <h1><font face="TimesNewRoman" size=6>Spine Deformity</h1>
        <p><font face="TimesNewRoman" size=5> Long-term inappropriate upper body postures may cause the irreversible deformation in spine curvature related to diseases such as kyphosis, lordosis and scoliosis, which are commonly treated with invasive surgery. In addition, even though clinical treatment can give patients immediate improvement of their symptoms, problematic upper body posture still needs to be addressed. Such chronic diseases are irreversible and hard to monitor due to multi-plane free motion ranges. One efficient way is to correct people's trunk posture by motions-constraint devices. Since these facts, detection and correction  of spine deformity are crucial on early stages, or, for a bigger picture, within our daily life.
        <br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/Corr.png" width="400px" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.1. Referenced images from <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s12178-011-9101-z">paper by Good et al.</a>. X-rays of a patient with progressive adult idiopathic scoliosis before and after thoracolumbar reconstruction T10-pelvis. Pedicle screw correction on coronal and sagittal plane through anterior lumbar interbody fusion L4- Sacrum were shown in the second and fourth pictures, respectively</div>
        </div>
        <h1>Upper Body Postures Monitoring Device</h1>
        <p><font face="TimesNewRoman" size=5> In this study, we try to develop an "Intelligent cloth system", which is a wearable device for long-term inadequate postures detection. The idea is straightforward, cloths are not only fit to the upper body for a long period but also necessary for people to use in their daily life, like how cell phones are. Since that, the basic idea is to put inertial sensors, which can acquire kinematic data related to upper body motions, onto the clothing system. The sensor part would be discussed at later section.
        <br><br>
        Seven common upper body "sitting" postures are interested: Humpback, upright, lean, left/right trunk bending, and left/right twisting. Sitting is the most common activities for people on weekdays and weekends, accounts for almost 70% of their days, according to <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1186/s12889-019-6960-5">this study by Arundell et al.</a> As a result, develop a device to monitor the sitting postures can help clinicians understand patients' behaviors. In addition, we designed all of these motions as single-axis rotation which are related to spine deformity, and it's more easier to control the human trials.
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/Pos.png" width="400px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.2. Seven common sitting postures are our targets: humpback, upright, lean, R/L bending, and R/L twisting</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/Cloth.png" width="400px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.3. The prime intelligent cloth system design, composed of multiple inertial sensors, and better fits to spine curvature</div>
        </div>
        <h1>Inertial Measurement Units</h1>
        <p><font face="TimesNewRoman" size=5> The sensors we used in this study is microelectromechanical systems (MEMS) 9-axis inertial measurement units(IMUs), composed of 3-axis accelerometer, gyroscope and magnetometer. IMUs have been widely used in global navigation system, for example, they are embedded into mobile phone to provide another modality of information of referenced system and corporated with GPS. A more easier understandable case is turning off GPS and your phone can still tell you where you "approximately" are and which direction you are facing. (They are both IMUs' magic!) The keys to provide such locational information are IMUs' signals-acceleration, angular velocity, and magnetic field-which are all refereced to global reference system.
        <br><br>
        However, all IMUs can provide are these three kinds of data, rather than location or orientation. The connection is called Attitude & Heading Reference Systems(AHRS). AHRS is a sensor fusion algorithm applying gravity and earth magnetic field referenced vector to compensate the drift by gyroscope and giving drift-free orientation. AHRS is a more cost effective solution than conventional high-grade IMUs that only integrate gyroscopes and rely on a very high bias stability of the gyroscopes. The attitude information on this sensor,
        including roll, pitch, and yaw, is beneficial for tasks about "where the body faced", like navigation.
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/AHRS.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.4. Block diagram of the fusion algorithm for AHRS published by <a target="_blank" rel="noopener" href="https://www.hindawi.com/journals/tswj/2014/597180/">Amin et al.</a> The last three outputs are euler angle of IMU's orientation</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/sys.png" height="300px" width="500px"  /></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.5. Four IMUs are uniformly distributed on spinal curvature and aligned on sagittal plane from T2 to L5(approximately)</div>
        </div>
        <h1>Human Trials</h1>
        <p><font face="TimesNewRoman" size=5> One contribution in this study is to construct dataset by only using single modality to collect trunk kinematic data. The human trials worked and followed this procedure: trial people would be instructed to do 7 common sitting postures mentioned above sequentially and continually. Each states were kept and changed from last state during five seconds. In other words, people need to undergo "transient" and "static" phases within five seconds and move to next run. There total 28 runs, which contain 4 runs for every postures, in single person's trial. The sampling rate of four IMUs is 50Hz.
        <br><br>
        By the characteristics of this transient-static and alternative trials, we can simply extract each phases. Then phases would be labeled automatically for supervised learning.
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/sig.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.4. All signal channals during single trials
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/det.png" height="300px" width="500px"  /></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.5. First-order differentiation of acceleration is applied to extract 28 intermittent phases and seperated as static(orange line) and transient phase</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/dataset.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.6. Label composition in this dataset</div>
        </div>
        <h1>Sitting Postures Recognition by Using RNN</h1>
        <p><font face="TimesNewRoman" size=5> For sitting postures recognition, we used LSTM-based recurrent neural networks(RNN) to do sequence-to-sequence classification. The innovation of RNN is "memomry". By the special structure with memory cells in network, which can extract time-domain features on signals, RNN has been widely used to deal with sequential problems, e.g. translation, signal prediction, objective detection, and so on. Long short-term memory(LSTM) network has getting more and more popular in recent year. LSTM contains four "gates" in the structure, including input, output, memory, and "forget" gate. The forget gate, which was designed to format the memory cell, turns out to be of the greatest importance by <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">Google's research</a>
        <br><br>
        There are two LSTM-based layers in the deep learning structure in this study. Each of them contains 128 LSTM units to better extract information on temporal domain. The input feature vector is 36-dimension, with 9-axis raw signals on four sensors, and the output is one of the seven classes. 
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/lstm.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.7. <a target="_blank" rel="noopener" href="https://developer.nvidia.com/discover/lstm">Schematic diagram of LSTM units.</a> Each LSTM unit contains four scalable gates: input, output, memory, and forget gate</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/NN.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.8. Proposed neural networks in this study with two layers containing 128 LSTM units</div>
        </div>
        <h1>Results</h1>
        <p><font face="TimesNewRoman" size=5> The final accuracy in training is up to 99.0% in 600 epoch. F1-score is averaged around 0.966. These pivot results showed practical and potential usage of IMUs-based application with LSTM-RNN. We test another trial as testing sequence to this trained model, and the accuracy is 81.2%, which shows a slight overfitting in the LSTM model. In addition, there are misclassification in transient phases, as the same observation in <a target="_blank" rel="noopener" href="http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf">paper by Rivera et al.</a> One potential solution is to use averaging filter in temporal domain to condense information in this short period, and we will try to fix this problem in future work.
        <br><br>
        Also, to cost down the whole system, we tried to find the relavance between sensor numbers and classifier performances. The accuracy dropped as sensor numbers decrease, and we suggest three sensors as optimal series, with averaged accuracy 98.1%. Upper series(1st, 2nd, and 3rd) has higher training accuracy than lower series, we think the reason is the 4th sensor, which located on almost L5, is static during trunk bending and humpback. This caused classifier trained with lower series is less robust than with the upper one and indicated locations of sensors significantly influence the performance of this application.  
        <br><br>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/exp.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.9. Setup in traini    ng procedure</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/confu.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.10. Confusion matrix and stastical result using four sensors</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/tran.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.11. Performance on sequential classification of testing trials. Green block indicates misclassification on transient phases</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/Posture/num.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.12. Performance of the same neural network using different sensor series</div>
        </div>
    </body>
</html>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/AoI"> Research</a></li><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Exp"> Experience</a></li><li class="nav_item"><a class="nav-page" href="/Contact"> Contact</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 by Louis Tang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>