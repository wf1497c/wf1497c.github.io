<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Louis Tang</title><meta name="author" content="Louis Tang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Louis Tang</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/AoI"> Research</a></li><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Exp"> Experience</a></li><li class="menus_item"><a class="site-page" href="/Contact"> Contact</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Louis Tang</h3><p class="author-bio">Research Personal Website</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><html>
    <head>
        <style>
            .flex {
              display: flex;
              padding: 5px;
              <!--background-color: #003C9D;-->
              // 修改以下值試試看 row | row-reverse | column | column-reverse;
              flex-direction: row;
              flex-wrap: wrap;
              word-wrap:break-word;
            }
            .item {
                width: 450px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: top;
                justify-content: left;
                color: black;
                font-size: 2rem;
                <!-- background-color: #4F4F4F; -->
            }
            .imgdes {
                width: 360px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: center;
                justify-content: left;
                color: black;
                font-size: 2rem;
                text-align: left;
                word-wrap:break-word
                <!-- background-color: #4F4F4F; -->
            }
            .item2 {
                height: 400px;
                width: 500px;
                background-color: #5B00AE;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                line-height: 30px;
            }
            .item3 {
                height: 400px;
                width: 500px;
                background-color: #4F9D9D;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                padding-right: 20px;
                line-height: 30px;
            }
            h2 {
                margin: 0;
            }
            hr {
                margin-top: 20px;
                margin-bottom: 50px;
                margin-right: 20px;
            }
            a {
                 style="color:red;"
            }
            li {
              color: black;
              font-size: 1em;
              list-style:none
            }
            ul {
                margin: 0px;
            }
            br {
                line-height: 10%;
                font-size: 24%;
            }
            .page article {
                padding-left: 70px;
                padding-right: 0px;
            }
            h1 {
                margin-bottom: 30px;
            }         
            p{
              word-wrap:break-word; 
              width: 900;
            }   
        </style>
    </head>
    <body>
        <h1><font face="TimesNewRoman" size=7> COVID-19 Patient Chest X-ray Images </font></h1>
        <hr></hr>
        <p><font face="TimesNewRoman" size=5>(Watch my journal seminar presentation at TMU to understand this topic ↓)
        <br></br>
        <iframe src="https://drive.google.com/file/d/1oiTyFDTkXZXd1q5wdsC7pRpU4rvUj3fs/preview" scrolling="no" frameborder="0" width="100%" height="430" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>
        <h1><font face="TimesNewRoman" size=6>COVIDx</h1>
        <p><font face="TimesNewRoman" size=5> Coronavirus 2019(COVID-19), which has destroyed global and hundreds of national health systems, has been one of the most disastrous challenges in human history. Because of its fast-widespread speed, the most crucial issue is diagnosis-whether a person is a COVID carrier or not. There are many methods to do diagnosis by detecting abnormal physiological responses on people, including temperature, blood test, and distribution of the lesions in chest radiography. Therefore, it’s necessary to set up quick detection system to each of the conditions to stop more spread of the pandemic diseases. Base on the urgent needs, Wang et. al. created a dataset called COVIDx, comprised of a total of 13,975 CXR images across 13,870 patient cases, is the the largest open access benchmark dataset available in terms of the number of publicly available COVID-19 positive cases. The propose in the study of COVIDx is to build robust AI model for COVID-19 detection via CXR modality, however, the scarcity of COVID-19 infected cases still limited its application. Therefore, data augmentation might be needed in some cases.
        <br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/COVIDx.jpg" width="400px" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.1. Referenced images from <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-020-76550-z">COVID-net's original paper.</a> Example CXR images from the COVIDx dataset, which comprises of 13,975 CXR images across 13,870 patient cases from five open access data repositories: (1) COVID-19 Image Data Collection16, (2) Figure 1 COVID-19 Chest X-Ray Dataset Initiative17 (established with Fig. 1), (3) RSNA Pneumonia Detection challenge dataset20, (4) ActualMed COVID-19 Chest X-Ray Dataset Initiative18 (established with ActualMed), and (5) COVID-19 radiography database19.</div>
        </div>
        <h1>Data Augmentation</h1>
        <p><font face="TimesNewRoman" size=5> In practice, the primary problem of building machine learning model in solving biomedical issues is data number. For example, when collecting data to build simple binary classification model, i.e. w/wo Alzheimer, normal cases always be majority in datasets with even tens to hundereds times of target numbers. This would bring an imbalanced dataset, which means the class distributions are biased to specific class-often normal cases- and influence the poor performance of classification model in predicting the minority target class. The ratio of pathological classes to normal is emphasized in the biomedical issues because the abnormalities are always the clinical target of interests. Also, misclassifications of the minority target class would result in serious consequences in detecting diseases, e.g. COVID-19 carriers are detected as negative, therefore, the balance of each classes within dataset must be carefully described. 
        <br><br>
        One common technique to solve imbalanced data problem is data augmentation, as the name states, to increase the data numbers in the datasets. Take image modality as example, transformation of images is commonly used to do data augmentation. Affine transformations in 2D, e.g. scaling, translation, rotation, shearing, and so on, preserves geometric characteristics like points, straight lines, and planes by spatial transforming of coordinates. Photometric transformations, with manipulation of color characteristics of RGB images by applying filters or changing histogram, is another data augmentation strategy using intrinsic representation of color in digital images. With these two techniques, we can augment the image numbers in dataset with the same class. However, a disadvantege of them is increased memory, transformation costs, and training time. As we known, geometric transformation is implementation of matrix transformation, more calculation resources of preprocessing in images, especially enormous datasets, would cause the inefficiency of training procedure. In addition, color transformation would sometimes discard crucial color characteristics of images in our classification proposes. 
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/Me.png" width="400px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.2. Examples of Color Augmentations provided by <a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Agnieszka_Mikolajczyk4/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem/links/5d5d5569458515210257607c/Data-augmentation-for-improving-deep-learning-in-image-classification-problem.pdf">Mikolajczyk and Grochowski</a> in the domain of melanoma classification</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/Aug.png" width="400px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.3. Affine transformations in 2D: reflection, scale, rotation, shear, and translation. With this manipulation, the image number increase from one to six.</div>
        </div>
        <h1>Generative Adversarial Networks</h1>
        <p><font face="TimesNewRoman" size=5> Generative adversarial Networks(GAN) is a novel tool to perform images generation. GANs has been used to different images manipulation, including text-to-image synthesis, image translation, style fusion, image reconstruction, and also, data augmentation. The overall principle of GANs is a min-max strategy. There are two adversarial parts in GANs structure: generator and discriminator. Generator(G(z)) is commonly a decoder network which can decode a low-dimension vector with random noises to synthesize "fake" or real-like images. Discriminator(D(z)) is a classifier(commonly a network) to discriminate an input image as "fake" or real. The connection between this two distinctive networks is their adversarial interactions: generator's task is to "fool" the discriminator and discriminator's task is to "not be fooled" by generator, as the interactions between a counterfeit maker and a police. In the aspects of training, optimize GANs could be more difficult than other typical neural networks because of the trade-off between abilities of generator and discriminator. 
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/GAN.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.4. GAN structure contains two adversarial networks: discriminator and generator, both influence optimization strategy</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/GAN2.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.5. Algorithm of GAN states objective function of the generator and discriminator</div>
        </div>
        <h1>Prospective Study</h1>
        <p><font face="TimesNewRoman" size=5> In the end, GANs were used to synthesize hundereds of COVID-19 patient chest x-ray images. In future work, COVID-19 classifier can be trained with these synthesized images by GANs and the performance would be compared to different data augmentation technique. Menon et al. reveiled the potential usage of this GAN-based transfer learning with chest x-ray images to detect COVID-19. I strongly believe this GAN-based classifier not only summarized the information of x-ray modality but also help us to build more robust classifier to overcome COVID-19. Another similar <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8461430">paper published by Salehinejad et al.</a> states that their COVID-19 x-ray classifier has better performance on GAN-based hybrid dataset for 5 pathological classes. Their wonderful work inspired me to share this idea to TMU staff at our journal seminar. More investigation on interplays between medical images synthetic and classifier performance should be done. 
        <br><br>
        <div class="flex">
            <div class="item"><img src="/img/article/GAN/GAN3.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.6. Synthesized chest x-ray image(right) and real images(left)</div>
        </div>
    </body>
</html>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/AoI"> Research</a></li><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Exp"> Experience</a></li><li class="nav_item"><a class="nav-page" href="/Contact"> Contact</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2021 by Louis Tang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>