<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Louis Tang</title><meta name="author" content="Louis Tang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Louis Tang</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/AoI"> Research</a></li><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Exp"> Experience</a></li><li class="menus_item"><a class="site-page" href="/Contact"> Contact</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Louis Tang</h3><p class="author-bio">Research Personal Website</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><html>
    <head>
        <style>
            .flex {
              display: flex;
              padding: 5px;
              <!--background-color: #003C9D;-->
              // 修改以下值試試看 row | row-reverse | column | column-reverse;
              flex-direction: row;
              flex-wrap: wrap;
              word-wrap:break-word;
            }
            .item {
                width: 450px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: top;
                justify-content: left;
                color: black;
                font-size: 2rem;
                <!-- background-color: #4F4F4F; -->
            }
            .imgdes {
                width: 360px;
                margin: 20px;
                margin-bottom: 50px;
                display: flex;
                align-items: center;
                justify-content: left;
                color: black;
                font-size: 2rem;
                text-align: left;
                word-wrap:break-word
                <!-- background-color: #4F4F4F; -->
            }
            .item2 {
                height: 400px;
                width: 500px;
                background-color: #5B00AE;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                line-height: 30px;
            }
            .item3 {
                height: 400px;
                width: 500px;
                background-color: #4F9D9D;
                margin: 0px;
                display: inline-flex;
                justify-content: center;
                align-items: center;
                color: white;
                font-size: 2rem;
                padding-left: 20px;
                padding-right: 20px;
                line-height: 30px;
            }
            h2 {
                margin: 0;
            }
            hr {
                margin-top: 20px;
                margin-bottom: 50px;
                margin-right: 20px;
            }
            a {
                 style="color:red;"
            }
            li {
              color: black;
              font-size: 1em;
              list-style:none
            }
            ul {
                margin: 0px;
            }
            br {
                line-height: 10%;
                font-size: 24%;
            }
            .page article {
                padding-left: 70px;
                padding-right: 0px;
            }
            h1 {
                margin-bottom: 30px;
            }         
            p{
              word-wrap:break-word; 
              width: 900;
            }   
        </style>
    </head>
    <body>
        <h1><font face="TimesNewRoman" size=7> [Article Reading] Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction </font></h1>
        <hr></hr>
        <p><font face="TimesNewRoman" size=5>(Watch IBM's wonderful idea to understand this topic ↓)</p>
        <iframe src="https://www.youtube.com/embed/i2-LgHjgDTs" scrolling="no" frameborder="0" width="100%" height="600" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>
        <p><font face="TimesNewRoman" size=5>(Watch my presentation at GIBMTE's seminar to understand this topic ↓)</p>
        Coming soon..
        <p><font face="TimesNewRoman" size=5>(觀看我的中文版報告來了解這個題目 ↓)</p>
        Coming soon..
        <h1><font face="TimesNewRoman" size=6>Organic Synthesis</h1>
        <p><font face="TimesNewRoman" size=5> Organic synthesis, the making of complex molecules from simpler building blocks, remains one of the key stumbling blocks in drug discovery. We can simply take every chemical reaction as specific combination of reactants, which refered to chemical substances that react with another, reagent, which is defined as a chemical species that does not appear in the product but involve in the reaction, and conditions, like temperature, pressure, and so on. Such flexible combination makes prediction of chemical reaction, especially organic chemistry, complicated and time-consuming. According to a previous <a target="_blank" rel="noopener" href="https://www.nature.com/articles/nrd.2018.116">paper</a>, there are estimated 10<sup>60</sup> feasible drug-like compounds to discovery. Since that, a better and quicker solution to predict chemical reaction is necessary for drug developer to accelerate drug discovery.
        <br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/Org.jpg" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.1. Simple illustration on how different conditions, reagent, and reactants would bring serial chemical reaction. Referenced images from <a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Organic-synthesis-of-Uros-A-B-and-C_fig1_315462603">this paper.</a></div>
        </div>
        <h1>Neural Machine Translation</h1>
        <p><font face="TimesNewRoman" size=5> In this study, IBM Research team was inspired by the idea of Neural Machine Translation(NMT) tasks and tried to find a solution of chemical reaction prediction. NMT, as its name states, aims at building neural network that can be jointly tuned to tackle the translation problem, according to <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">this paper.</a> We are all familiar with translation: giving a sentence with unknow meaning to a translator, you will accept a sentence of interest, i.e. the sentence you might understand or want to learn. However, to build a translator is not easy because it's not a simple one word in-and-out problem, the positions of words, orders of words, meaning of single word, and even the relationship between words and the sentence should be taken into account. Basiclly, neural network, compared to traditional statistical machine translation, can extract latent features or patterns and condense these information into deeper knowledge space. As a result, for the complicated task of translation, neural network has been widely used to investigate its related application. And the common model used in this application is called Sequence-to-sequence model(S2S) because of the characteristic of sequential in-and-out. 
        <br><br>
        The biggest question here is: Why is translation related to chemical reaction? The answer is: taking reactant and reagent as "sentence". Simplified molecular-input line-entry system (SMILES) is a global-used text representation of chemical molecules. For example, ethanol can be simplified as CCO, OCC, and C(O)C in this form. Follow this thought, if we tranform reactants and reagent within our proposed chemical reaction into SMILES, and passed them into a "translator", can we get another "sentence", or we say, another SMILES, which represented the product? Fig.2. simply present this idea and it is the whole understandable NMT structure in this paper. 
        <br><br>
        The key within this connection is the sequence, which means things(words and elements in these two cases) arranged in a particular order, i.e. change the order, change the essence in the sequence. And the order is what we need to extract features from by a special neural network.
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/tran.png" height="350px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.2. The translation analogy to chemical reaction. In comparison, what we need to translate is not a sentence but a SMILES representation by a neural network called "Transformer"</div>
        </div>
        <h1>Recurrent Neural Network</h1>
        <p><font face="TimesNewRoman" size=5> As how people memorize things, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/650093">Recurrent Neural Network(RNN)</a> provides a way to deal with sequential data emboding correlations between neighbor data input in different timestep. The structural explanation is that within each timestep in RNN, the input is not only a single component of the source sequence but the hidden state in previous timestep, i.e. information in previous state, which represented the recurrence in the network, and each timestep RNN can give an output, as a word in a translated sentence. With this recurrent structure, RNN can memorize sequential information by this information-cumulatived mechanism.
        <br><br>
        However, this mechanism is a potential problem in RNN encoder-decoder model. It is simple to infer that the end of input sequence is more related to input in decoder's initial timestep, like how human "forget" events in long time ago, as a result, translated sentence would focus more on the end of input rather than consider the whole maening of sentence. This phenomonon is called "inductive bias" in RNN. Since that, couple of improved structure was designed, like Long Short-Term Memory(LSTM) cell and Gated Recurrent Unit.
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/RNN.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.3. <a target="_blank" rel="noopener" href="https://medium.com/ai-academy-taiwan/attention-mechanism-fad735db3c2c">Schematic diagram</a> of a encoder-decoder RNN structure. The encoder would encode the input sequence into a context vector, which contains sequential information and represents the whole sentence, and the decoder decodes it into an output sequence.</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/lstm.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.4. <a target="_blank" rel="noopener" href="https://developer.nvidia.com/discover/lstmFour">Schematic diagram</a> of LSTM cell. Four gates control one LSTM unit and determine whether previous information can be passed to next state or not: input-, output-, forget- and memory-gate. The <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">forget gate</a> was found to be the most crucial in RNN performance by Google.</div>
        </div>
        <h1>Attention Mechanism</h1>
        <p><font face="TimesNewRoman" size=5> Another solution for NMT is attention mechanism. For an encoder-decoder generative RNN structure, previous work used context vector as decoder's input, and this served as the rational in inductive bias. In comparison, input vector in each timestep in attention-based NN is "special". These input vectors are determined by attention mechanism, which allows modeling of dependencies without regard to their distance in the input or output sequences, i.e. each output component should consider the whole input sequence in an attending way. By this structure, RNN can handle long sequential translation by attending the whole input sequence in every output.
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/AM.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.5. <a target="_blank" rel="noopener" href="https://medium.com/ai-academy-taiwan/attention-mechanism-fad735db3c2c">Schematic diagram</a> of a encoder-decoder RNN structure using attention mechanism. The decoder would attend the input sequence in every timestep(E0~En)</div>
        </div>    
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/AMV.jpg" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.6. <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/xuc15.pdf">Attention mechanism in image translation</a>. With attetntion, RNN can give a pristine description on the input image by attending on different parts of the image</div>
        </div>   
        <h1>Self-Attention: Transformer</h1>
        <p><font face="TimesNewRoman" size=5> A paper called <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">"Attention Is What You Need"</a> was published by Google Brain and they named this model as "Transformer". They designed a self-attention-based structure which can to tackle NMT tasks in parallel by three distinctive parts. First, Scaled Dot-Product Attention, dividing attention into three sub-matrices: query(Q), key(K), and extracted information vector(V), can help match attention more efficiently. The Scaled Dot-Product Attentions are formed by inner product(or other form) of Q and K, and scaled by a dim length-factor. Second, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely, as a reslut, decoder can rely the global dependencies in input sequence. Positional Encoding makes up the initial recurrent parts in RNN to give “where” concept. Third, Multi-Head Attention divides QKV into multiple parts to see different parts of seq., e.g. local or global. 
        <br><br>
        The Molecular Transformer, based on six layer Transformer, was used in this study. Also they used scaled dot-product attention and multi-head attention and decreased the parameters from 65M to 12M.
        <br><br>
        </p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/Trm.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.7. The strucutre of single layer Transformer with scaled dot-product attention and multi-head attention </div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/head.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.8. The attention weight matrix in a translation task. Multi-head attention makes Transformer to attend on different parts of input</div>
        </div>
        <h1>Results in Chemical Reaction Prediction</h1>
        <p><font face="TimesNewRoman" size=5> Their baseline, i.e. no data augmentation and checkpoints averaged weight, top-1 accuracy is 88.3% which outperforms other NN model, including S2S and WLDN(GNN-based), on different dataset. Also, their model proved that they can predict in "mixture SMILES", i.e. composed of reactants and reagent by directly adjoin their SMILES, and still outperform other works with seperated SMILES. This showed that their model is more convenient for chemist to deal with prediction without identifing reagent, which sometimes is difficult to do. In addition, compared to state-of-art model in regioselectivity-RegioSQM, Molecular Transformer, which has trained in diverse reaction types, still outperform in this specific reaction type prediction, with 81% and 83% top-1 accuracy, respectively.
        <br><br>
        The most imporatant part of it success is the Transformer, which handled long sequential problem very well. Also multi-head attetntion helped model to find importances of information in different parts, as the way chemist consider the reaction in some relevance between molecules, e.g. functional group. Compared to human experts, the Molecular Transformer outperform on USPTO_MIT dataset with 87.5% accuracy, and human with 76.5%. In addition, the Molecular Transformer also found some subtle products, which human failed to observe, and make chemical-sense. This proved the potential usage of this model to help acceleration of drug discovery by predicting these subtle products. 
        <br><br>
        In the end of this paper, they also evaluate the uncertainty of model's estimation to prove its abilities of handle different length of input. The basic idea is give every estimation a confidence score and quantify the Pearson correlation coefficient between this confidence score and the input length. The result is 0.06, which indicated no correlation within these two indice. In other words, Molecular Transformer can handle long sequence without losing significant ability.</p>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res1.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.9. Resluts: model performance. Machine learning techniques like Ensemble, checkpoimts averaged weight, and data augmentation were used in this study</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res2.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.10. Results: model performance related to other NN model, including S2S and GNN-based algorithm. Also, the influence by template popularity, i.e. the number of times a particular reaction type is seen in the data set, was investigated</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res3.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.11. Results: model performance compared to Quantum-Chemistry Model, which is a state-of-art model in regioselectivity prediction problem</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res4.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.12. Results: model performance compared to 11 human chemists. The model not only outperformed human experts but also observed subtle products, as figure shown</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res5.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.13. Results: model performance compared to S2S model</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res6.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.14. Results: model performance in different reaction types. The worst result is in resolution prediction, as S2S model performed</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res7.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.15. Results: model performance in uncertainty estimation. This result showed that the Molecular Transformer can predict long sequence of reaction input without losing too much abilities</div>
        </div>
        <div class="flex">
            <div class="item"><img src="/img/article/IBM/res8.png" height="300px"/></div>
            <div class="imgdes"><font face="TimesNewRoman" size=4>Fig.16. Conclusion in this study</div>
        </div>
    </body>
</html>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/AoI"> Research</a></li><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Exp"> Experience</a></li><li class="nav_item"><a class="nav-page" href="/Contact"> Contact</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2021 by Louis Tang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>